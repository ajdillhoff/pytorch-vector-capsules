{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Routing Between Capsules\n",
    "\n",
    "This notebook implements and demonstrates the method introduced in this paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How the vector inputs and outputs of a capsule are computed\n",
    "\n",
    "The length of an output vector represents the probability that the entity represented by the capsule is present given the input. To do this, a squashing function is introduced that maps the output between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005314350128173828\n",
      "0.0005106925964355469\n",
      "0.00038743019104003906\n",
      "tensor([[[0.1753, 0.1390],\n",
      "         [0.0590, 0.2599],\n",
      "         [0.3021, 0.0313],\n",
      "         [0.2541, 0.0536]]])\n",
      "tensor([[[0.1753, 0.1390],\n",
      "         [0.0590, 0.2599],\n",
      "         [0.3021, 0.0313],\n",
      "         [0.2541, 0.0536]]])\n",
      "tensor([[[0.1753, 0.1390],\n",
      "         [0.0590, 0.2599],\n",
      "         [0.3021, 0.0313],\n",
      "         [0.2541, 0.0536]]])\n"
     ]
    }
   ],
   "source": [
    "def squash1(x):\n",
    "    \"\"\"https://github.com/higgsfield/Capsule-Network-Tutorial/blob/master/Capsule%20Network.ipynb\"\"\"\n",
    "    squared_norm = (x ** 2).sum(-1, keepdim=True)\n",
    "    return squared_norm * x / ((1. + squared_norm) * torch.sqrt(squared_norm))\n",
    "\n",
    "def squash2(x):\n",
    "    \"\"\"My interpretation.\"\"\"\n",
    "    x_norm = x.norm(dim=-1, keepdim=True) ** 2\n",
    "    return (x_norm / (1 + x_norm)) * (x / torch.sqrt(x_norm))\n",
    "\n",
    "def squash3(x, dim=-1):\n",
    "    \"\"\"https://github.com/gram-ai/capsule-networks/blob/master/capsule_network.py\"\"\"\n",
    "    squared_norm = (x ** 2).sum(dim=dim, keepdim=True)\n",
    "    scale = squared_norm / (1 + squared_norm)\n",
    "    return scale * x / torch.sqrt(squared_norm)\n",
    "\n",
    "x = torch.rand(1, 4, 2)\n",
    "t1 = time.time()\n",
    "s1 = squash1(x)\n",
    "print(time.time() - t1)\n",
    "t1 = time.time()\n",
    "s2 = squash2(x)\n",
    "print(time.time() - t1)\n",
    "t1 = time.time()\n",
    "s3 = squash3(x)\n",
    "print(time.time() - t1)\n",
    "print(s1)\n",
    "print(s2)\n",
    "print(s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is `s` and where does it come from?\n",
    "\n",
    "`s_j` is the input to capsule *j*. It comes from a weighted sum of prediction vectors `u_hat` and coupling coefficients (more on that later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0669, 0.7517, 1.7444],\n",
      "        [1.5092, 0.5192, 1.4241]])\n",
      "tensor([[2.0669, 0.7517, 1.7444],\n",
      "        [1.5092, 0.5192, 1.4241]])\n",
      "tensor([[0.6534, 0.2376, 0.5514],\n",
      "        [0.5790, 0.1992, 0.5464]])\n"
     ]
    }
   ],
   "source": [
    "N = 4  # number of capsules in layer 0\n",
    "M = 2  # number of capsules in layer 1\n",
    "D = 3  # dimension of capsule in layer 0\n",
    "c = torch.rand(M, N)\n",
    "u_hat = torch.rand(N, D)\n",
    "\n",
    "s = torch.empty(M, D)\n",
    "\n",
    "# Calculating s_j\n",
    "for j in range(M):\n",
    "    for i in range(N):\n",
    "        s[j] += c[j,i] * u_hat[i]\n",
    "print(s)\n",
    "print(c @ u_hat)\n",
    "print(squash2(c @ u_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Routing\n",
    "\n",
    "Measure the agreement between the current output of each capsule in the higher layer with each prediction from the lower capsules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9684, 0.3446, 0.8407],\n",
      "        [0.6700, 0.0335, 0.7692],\n",
      "        [0.6673, 0.9354, 0.4818],\n",
      "        [0.8452, 0.3606, 0.5036]])\n",
      "tensor([[0.3920, 0.2083, 0.3229],\n",
      "        [0.3920, 0.2083, 0.3229]])\n"
     ]
    }
   ],
   "source": [
    "def routing(u_hat, r, l):\n",
    "    \"\"\"Computes the output vector for the capsule while updating the coupling\n",
    "    coefficients.\n",
    "    \n",
    "    Args:\n",
    "        u_hat (D): Prediction from lower level capsule.\n",
    "        r (int): Number of iterations.\n",
    "        l (int): Current layer.\n",
    "    Returns:\n",
    "        v (D): Output vector.\n",
    "    \"\"\"\n",
    "    b = torch.zeros(M, N)\n",
    "    \n",
    "    for i in range(r):\n",
    "        # for all capsule i in layer l\n",
    "        c = F.softmax(b, dim=-1)\n",
    "        \n",
    "        # for all capsule j in layer (l+1)\n",
    "        s = c @ u_hat\n",
    "        print(u_hat)\n",
    "        \n",
    "        # for all capsule j in layer (l+1)\n",
    "        v = squash2(s)\n",
    "        \n",
    "        if i < r - 1:\n",
    "            b += v @ u_hat.transpose(0, 1)\n",
    "            \n",
    "        return v\n",
    "        \n",
    "v = routing(u_hat, 3, 0)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Margin loss for digit existence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 1.],\n",
      "        [0., 0., 1., 1., 0., 0., 1., 1., 0., 0.],\n",
      "        [1., 1., 0., 0., 1., 0., 1., 0., 0., 1.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 1., 1., 0.]])\n",
      "tensor(4.7228)\n"
     ]
    }
   ],
   "source": [
    "m_plus = 0.9\n",
    "m_minus = 0.1\n",
    "lam = 0.5\n",
    "labels = torch.empty(4, 10, dtype=torch.float32).random_(2)\n",
    "print(labels)\n",
    "\n",
    "x = torch.rand(4, 10, 16)\n",
    "v_c = x.norm(p=2, dim=2, keepdim=True)\n",
    "left = F.relu(m_plus - v_c).view(4, -1)\n",
    "right = F.relu(v_c - m_minus).view(4, -1)\n",
    "loss = labels * left + lam * (1.0 - labels) * right\n",
    "loss = loss.sum(dim=1).mean()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primary Capsule\n",
    "\n",
    "Primary capsules consist of a convolutional layer with 32 channels of convolutional 8D capsules. This means that each primary capsule contains 8 conv units with a 9x9 kernel and stride 2. `PrimaryCapsules` has (32x6x6) capsule outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1152, 8])\n",
      "torch.Size([1, 10, 1152, 8, 16])\n",
      "torch.Size([4, 1, 1152, 1, 8])\n",
      "torch.Size([4, 10, 1152, 1, 16])\n"
     ]
    }
   ],
   "source": [
    "class PrimaryCapsule(nn.Module):\n",
    "    def __init__(self, num_capsules=8, in_channels=256, out_channels=32, kernel_size=9):\n",
    "        super(PrimaryCapsule, self).__init__()\n",
    "        # Initialize the convolutional capsules\n",
    "        self.capsules = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride=2, padding=0) for _ in range(num_capsules)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Prediction for each capsule\n",
    "        u = [capsule(x) for capsule in self.capsules]\n",
    "        u = torch.stack(u, dim=1)\n",
    "        u = u.view(x.size(0), 32 * 6 * 6, -1)\n",
    "        return squash2(u)\n",
    "        \n",
    "cap = PrimaryCapsule()\n",
    "conv1 = nn.Conv2d(1, 256, kernel_size=9, stride=1)\n",
    "x = torch.rand(4, 1, 28, 28)\n",
    "x = conv1.forward(x)\n",
    "x = cap(x)\n",
    "print(x.shape)\n",
    "w = torch.randn(10, 32 * 6 * 6, 8, 16)\n",
    "r = w[None, :, :, :, :]\n",
    "t = x[:, None, :, None, :]\n",
    "print(r.shape)\n",
    "print(t.shape)\n",
    "print((t@r).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit Capsule\n",
    "Digit capsules implement the routing mechanism to determine the part-whole relationships based on the predictions of the primary capsules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitCap(nn.Module):\n",
    "    def __init__(self, num_capsules=10, in_size=32 * 6 * 6, in_channels=8, out_channels=16,\n",
    "                num_iterations=3):\n",
    "        self.num_iterations = 3\n",
    "        self.W = nn.Parameter(torch.randn(num_capsules, in_size))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        pred = x[:, None, :, None, :] @ self.W[None, :, :, :, :]\n",
    "        pred = pred.squeeze(3)  # necessary?\n",
    "        logits = torch.zeros(pred.shape, device=x.device)\n",
    "        \n",
    "        for i in range(self.num_iterations):\n",
    "            probs = F.softmax(logits, dim=2)\n",
    "            outputs = squash2((probs * pred).sum(dim=2, keepdim=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch1.0)",
   "language": "python",
   "name": "pytorch1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
